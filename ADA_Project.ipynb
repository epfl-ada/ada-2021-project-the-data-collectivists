{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Alt Right Community](/img/ALT_RIGHT.jpg)](https://www.jstor.org/stable/26984798?seq=1#metadata_info_tab_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8XLEWsHRc-1"
   },
   "source": [
    "# The rise of far-right extremism speech between 2016 and 2020 \n",
    "### Observed through a dataset of quotes from the press, highlighting the evolution of opinions and ideas that shape the past, present, and the future of our society.\n",
    "\n",
    "$$ \\\\ $$\n",
    "\n",
    "Project in Applied Data Analysis (CS-401)\n",
    "\n",
    "*Team members: Camil Hamdane (SV), Clémentine lévy-Fidel (SV), Nathan Fiorellino (SV), Nathan Girard (SV)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. About the project \n",
    "\n",
    "The peace and prosperity that we encountered in developped countries of the North in the last century has led many nations in a path of constant technological progress and economic growth. This prosperity however might be threatened by newfound global issues that not only jeopardize the economy, but also the future of humanity. The recent COVID-19 pandemic has shown once more how the current socio-economic system ubiquitous in most occidental democracies has potentially fatal flaws that we need to address before it is crushed under its own weight.\n",
    "\n",
    "While most people agree about the uncertainty of the next hundred years, we still have yet to agree on a solution. Some of which are more oriented towards a progressive society, while others prefer a more conservative approach. While multiple point of view rely on economic and environmental claims to base their theories on, some others are based on hate and fear of the difference, hate and fear of the change we might need to forge a more inclusive society. In order to better tackle the issues we face, we need to understand how some ideologies are gaining more traction inside the public debate, to observe how they might shape the minds of citizens.\n",
    "\n",
    "We are interested today about the **rise of far-right extremism** speech between **2016** and **2020**, observed through a dataset of quotes from the press, highlighting the evolution of opinions and ideas that shape the past, present, and the future of our society.\n",
    "\n",
    "With that in mind, we hope to answer the following questions:\n",
    "- Is far-right extremism speech on the rise since 2016 ? \n",
    "- How accurately can we identify a trend with the Quotebank dataset, when put in perspective with right-wing extremist terrorist attacks ? \n",
    "- Can we highlight some news outlets that spread hate speech more than other, and if so, is it consistent over the years ? \n",
    "- Is it also consistent with their political opinion ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oA_fe4oxWT6p"
   },
   "source": [
    "## 2. How to get started\n",
    "\n",
    "### 2.1 Mount your drive to your notebook\n",
    "\n",
    "It is possible to mount your Google Drive to Colab if you need additional storage or if you need to use files from it. To do that run (click on play button or use keyboard shortcut 'Command/Ctrl+Enter') the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25604,
     "status": "ok",
     "timestamp": 1636464629770,
     "user": {
      "displayName": "",
      "photoUrl": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "CEcIlRwfWY4C",
    "outputId": "9bb072d9-2ed6-43c0-ff94-9f55cea61c63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Nfr-aXhWbFk"
   },
   "source": [
    " \n",
    "\n",
    "1.   After running the cell, URL will appear.\n",
    "\n",
    "2.   Following this URL, you will be redirected to the page where you need to choose Google Drive account to mount to.\n",
    "\n",
    "3.   You will further be asked to give Google Drive Stream a permission to access the chosen Google account\n",
    "\n",
    "4.   After granting the access, authorization code will be given to you\n",
    "\n",
    "5.   Copy the authorization code into the dedicated textbox in Colab under '*Enter your authorization code:*' writing\n",
    "\n",
    "After copying the authorization code, you should get the message saying '*Mounted at /content/gdrive*'\n",
    "\n",
    "Path to the files from the mounted Drive will then be '/content/drive/MyDrive/'. By opening the Files tab (left sidebar, folder icon) you should also be able to see the accessible files. Now, you can read the data directly from the Google Drive you mounted following the process above. Make sure you mounted the drive to which you saved the shortcut to the Quotebank data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Install the required libraries (*OPTIONAL*)\n",
    "\n",
    "You don't need to perform this step if you have previously installed the libraries ([pandas](https://pandas.pydata.org), [seaborn](https://seaborn.pydata.org), [tld](https://pypi.org/project/tld/)) on an environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.0.5 in /opt/anaconda3/envs/ada/lib/python3.8/site-packages (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/anaconda3/envs/ada/lib/python3.8/site-packages (from pandas==1.0.5) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/anaconda3/envs/ada/lib/python3.8/site-packages (from pandas==1.0.5) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda3/envs/ada/lib/python3.8/site-packages (from pandas==1.0.5) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/ada/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas==1.0.5) (1.16.0)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/envs/ada/lib/python3.8/site-packages (0.11.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/anaconda3/envs/ada/lib/python3.8/site-packages (from seaborn) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/anaconda3/envs/ada/lib/python3.8/site-packages (from seaborn) (1.20.3)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /opt/anaconda3/envs/ada/lib/python3.8/site-packages (from seaborn) (3.4.2)\n",
      "Requirement already satisfied: pandas>=0.23 in /opt/anaconda3/envs/ada/lib/python3.8/site-packages (from seaborn) (1.0.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/envs/ada/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (8.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/anaconda3/envs/ada/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/ada/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/ada/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/envs/ada/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/ada/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/anaconda3/envs/ada/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.1)\n",
      "Requirement already satisfied: tld in /opt/anaconda3/envs/ada/lib/python3.8/site-packages (0.12.6)\n"
     ]
    }
   ],
   "source": [
    "# Installations\n",
    "!pip install pandas==1.0.5 \n",
    "!pip install seaborn\n",
    "!pip install tld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jk/brc5_bwj7cbbtk3n69jp0mg80000gn/T/ipykernel_5795/3889555661.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKUy29h68N0J"
   },
   "source": [
    "## 3. Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants \n",
    "path_to_file = '/content/drive/MyDrive/Quotebank/quotes-2020.json.bz2'\n",
    "new_filename = 'processed_quotes-2020'\n",
    "csv_extension = '.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DK4YmTdA9wmY"
   },
   "source": [
    "### 3.1 Extraction of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2V67aCknDjgY"
   },
   "source": [
    "In this section, you can extract the data and preprocess it, such that you can conduct a correct analysis. Also, this step enables you to help the model to understand more easily this data, hence giving better answer to our questions. To this extent, the filtering step are displayed below:\n",
    "\n",
    "- Remove news outlets relaying less than XXXXX quotes that do not contribute to reflecting global trends,\n",
    "- Remove quotes with low numOccurences (less than **10**),\n",
    "- Filter for samples with only **1** QIDs,\n",
    "- Filter quotations that mentions keywords that we choose to tackle.\n",
    "\n",
    "Because of the huge size of the data and the limited capacity of the RAM provided by Google, we decided to process the data **per year**. In addition, chunks of data (per year) are processed sequentially to avoid exceed storage capacity. This preprocessing pipeline filters most of the data, such that **approximately 4%** of the initial data remains at the end. We obtain therefore clean and usable data for further analysis, stored in files named *\"processed_quotes-20XX.csv\"*, for each year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aHMv0FD0Eh_W"
   },
   "outputs": [],
   "source": [
    "# Definition of helper functions\n",
    "\n",
    "def clean_chunk(chunk):\n",
    "    \"\"\" \n",
    "        Cleans dataset chunk by removing unattributed quotes (quotes whose most probable speaker is unknown) or quotes whose speaker name is associated with more \n",
    "        than 1 alias removes 'probas' column and keep only quotes whose speaker probability is greater than 0.6 removes 'phase' column.\n",
    "    \"\"\"\n",
    "    # TO-ADD: remove quotes whose occurences number is smaller than 10\n",
    "          \n",
    "    chunk_clean = chunk.copy()\n",
    "    \n",
    "    # Filtering for samples containing exactly one QIDs\n",
    "    chunk_clean = chunk_clean.iloc[chunk[chunk['qids'].map(len) == 1].index]\n",
    "    chunk_clean['qids'] = chunk_clean['qids'].apply(lambda qids: qids[0])\n",
    "    \n",
    "    # Remove samples with more than 1 speaker \n",
    "    if chunk_clean['probas'].dtype != 'float64':\n",
    "        chunk_clean['probas'] = chunk_clean['probas'].apply(lambda probas: float(probas[0][1]))\n",
    "    \n",
    "    chunk_clean = chunk_clean[chunk_clean['probas']>0.6]\n",
    "    \n",
    "    # Remove Samples with low numOccurences\n",
    "    chunk_clean = chunk_clean[chunk_clean['numOccurrences']>10]\n",
    "    \n",
    "    # Filtering \n",
    "    if 'phase' in chunk_clean:\n",
    "        chunk_clean = chunk_clean.drop('phase', axis = 1)\n",
    "        \n",
    "    return chunk_clean\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    \"\"\"\n",
    "        Print the size of the chunk processed and the names of the columns. \n",
    "    \"\"\"\n",
    "    \n",
    "    print(f'Processing chunk with {len(chunk)} rows')\n",
    "    print(chunk.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jk/brc5_bwj7cbbtk3n69jp0mg80000gn/T/ipykernel_5795/1590878130.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'bz2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_reader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mprocess_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ada/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;34m\"ms\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m31536000000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0;34m\"us\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m31536000000000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m         \u001b[0;34m\"ns\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m31536000000000000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m     }\n\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ada/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \"\"\"\n\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ada/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    883\u001b[0m         \"\"\"\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0;31m# don't try to coerce, unless a force conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_dtypes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ada/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_convert_dates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m         self._process_converter(\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "# Extraction of the data\n",
    "\n",
    "df_reader = pd.read_json(path_to_file, lines = True, compression = 'bz2', chunksize = 1000000)\n",
    "\n",
    "for i, chunk in enumerate(df_reader):\n",
    "    process_chunk(chunk)\n",
    "    chunk = clean_chunk(chunk)\n",
    "    \n",
    "    chunk_file_name =new_filename + csv_extension\n",
    "    chunk.to_csv(chunk_file_name, mode='a')\n",
    "    \n",
    "    files.download(chunk_file_name)\n",
    "    break\n",
    "    \n",
    "# TO-DO: aggregate chunks for 2020, aggreagate with years 2016-2019, and export data in bz2 or CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4 Processing **[TO BE DONE - P3]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of our models to improve the interpretability of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization **[TO BE DONE - P3]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ADA_Project.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/GBNTN/ADA_Project_2021/blob/main/Tutorial_notebook.ipynb",
     "timestamp": 1636470885222
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
