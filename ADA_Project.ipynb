{"cells":[{"cell_type":"markdown","metadata":{"id":"C4Z_2WLx-E-b"},"source":["[![Alt Right Community](/img/ALT_RIGHT.jpg)](https://www.jstor.org/stable/26984798?seq=1#metadata_info_tab_contents)"]},{"cell_type":"markdown","metadata":{"id":"p8XLEWsHRc-1"},"source":["# The rise of far-right extremism speech between 2016 and 2020 \n","### Observed through a dataset of quotes from the press, highlighting the evolution of opinions and ideas that shape the past, present, and the future of our society.\n","\n","$$ \\\\ $$\n","\n","Project in Applied Data Analysis (CS-401)\n","\n","*Team members: Camil Hamdane (SV), Clémentine lévy-Fidel (SV), Nathan Fiorellino (SV), Nathan Girard (SV)*\n","\n"]},{"cell_type":"markdown","source":["# **Section 1 : Wrangling**"],"metadata":{"id":"SOjzVozeJQgN"}},{"cell_type":"markdown","source":["## 1. Getting started\n","\n","### 1.1 Installing and importing libraries\n","\n"],"metadata":{"id":"GoDhS_iLyT3W"}},{"cell_type":"code","source":["# Installations\n","!pip install pandas==1.0.5 \n","!pip install seaborn\n","!pip install tld\n","!pip install numpy\n","!pip install matplotlib\n","!pip install vaderSentiment"],"metadata":{"id":"wmeNuTSDx9mX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Imports\n","import os\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","import ast\n","from urllib.parse import urlparse\n","import string\n","from string import punctuation"],"metadata":{"id":"u8AT8UMUyDBC","executionInfo":{"status":"ok","timestamp":1639387818381,"user_tz":-60,"elapsed":605,"user":{"displayName":"Nathan Fiorell","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11892712415418385245"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Google import\n","from google.colab import drive\n","from google.colab import files"],"metadata":{"id":"FKgYDqYSyyjt","executionInfo":{"status":"ok","timestamp":1639387819331,"user_tz":-60,"elapsed":13,"user":{"displayName":"Nathan Fiorell","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11892712415418385245"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### 1.2 Mounting drive"],"metadata":{"id":"QzFFwwPkyg7i"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CEcIlRwfWY4C","outputId":"d4321a4c-91e4-4c6d-bdcf-5c909bfe5efb","executionInfo":{"status":"ok","timestamp":1639387823508,"user_tz":-60,"elapsed":2255,"user":{"displayName":"Nathan Fiorell","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11892712415418385245"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"KRUmuW0b-E-y","executionInfo":{"status":"ok","timestamp":1639387824988,"user_tz":-60,"elapsed":357,"user":{"displayName":"Nathan Fiorell","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11892712415418385245"}}},"outputs":[],"source":["# Data Path \n","input_path = '/content/drive/MyDrive/Quotebank/'\n","data_storage_path = '/content/drive/Shareddrives/Ada-The data collectivists/'\n","\n","primer_input = \"quotes-\"\n","tail_input = \".json.bz2\"\n","\n","primer_output = \"df_FF\"\n","tail_output = \".csv\"\n","\n","years = [\"2016\", \"2017\", \"2018\", \"2019\", \"2020\"]"]},{"cell_type":"markdown","metadata":{"id":"cKUy29h68N0J"},"source":["## 2. Preprocessing\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DK4YmTdA9wmY"},"source":["### 2.1 Extraction of the data"]},{"cell_type":"markdown","metadata":{"id":"m7UOYT3pWV8h"},"source":["#### a. Data Exploration\n"]},{"cell_type":"markdown","source":["We first take a quick look at the data so that we can confirm that we read it correctly and that we get an  idea of what it looks like so we can better decide how we will preprocess and particularly what kind of filters are relevant."],"metadata":{"id":"TqbPGaJIJy1F"}},{"cell_type":"code","source":["path_to_file = input_path + primer_input + years[4] + tail_input"],"metadata":{"id":"XCj68Wn52n7j","executionInfo":{"status":"ok","timestamp":1639387827384,"user_tz":-60,"elapsed":356,"user":{"displayName":"Nathan Fiorell","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11892712415418385245"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["path_to_file"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"acT5gU_DFX-B","executionInfo":{"status":"ok","timestamp":1639387828262,"user_tz":-60,"elapsed":16,"user":{"displayName":"Nathan Fiorell","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11892712415418385245"}},"outputId":"8814d405-eb48-44bb-9d2b-f3c8fffe3ceb"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/MyDrive/Quotebank/quotes-2020.json.bz2'"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["df_reader = pd.read_json(path_to_file, lines = True, compression = 'bz2', chunksize = 100)"],"metadata":{"id":"LJG5v5Qr8urv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZBJppK5EXAmX"},"outputs":[],"source":["for chunk in df_reader:\n","  print(\"Columns\")\n","  print(chunk.columns)\n","  print(\"Colum 1 and 2\")\n","  print(chunk.iloc[0:5,0:2])\n","  print(\"Colum 3 and 4\")\n","  print(chunk.iloc[0:5,2:4])\n","  print(\"Colum 5 and 6\")\n","  print(chunk.iloc[0:5,4:6])\n","  print(\"Colum 7 and 8\")\n","  print(chunk.iloc[0:5,6:8])\n","  print(\"Colum 9\")\n","  print(chunk.iloc[0:5,8])\n","  fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n","\n","  sns.histplot(ax = axes[0], data = chunk['numOccurrences'], bins = 100)\n","  sns.histplot(ax = axes[1], data = chunk['date'], bins = 100)\n","  break"]},{"cell_type":"markdown","metadata":{"id":"2V67aCknDjgY"},"source":["In this section, we extract the data and preprocess it, such that we can better conduct our analysis. The Data exploration brought us to choose the following filtering steps:\n","\n","- Remove quotes with low numOccurences, less than **10**,\n","- Remove quotes that have more than **1** speaker,\n","- Filter for samples with only **1** QID,\n","- Keep quotes whose probability is higher than **0.6**\n","- Remove **\"phase\"** feature of the dataframe\n","- Remove news outlets relaying less than a certain number of quotes, as they do not contribute to reflecting global trends.\n","\n","Because of the huge size of the data and the limited capacity of the RAM provided by Google, we decided to process the data **per year**. In addition, chunks of data (per year) are processed sequentially to avoid exceed storage capacity. This preprocessing pipeline filters most of the data, such that **approximately 4%** of the initial data remains at the end. We obtain therefore clean and usable data for further analysis, stored in files named *\"processed_quotes-20XX.csv\"*, for each year. "]},{"cell_type":"code","source":["# Constants\n","min_occurence = 10\n","acceptable_QID_amount = 1\n","min_probability = 0.6\n","remove_columns = [\"phase\"]"],"metadata":{"id":"KQt9u4hVzJBF","executionInfo":{"status":"ok","timestamp":1639351762777,"user_tz":-60,"elapsed":259,"user":{"displayName":"Nathan Fiorell","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11892712415418385245"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","execution_count":91,"metadata":{"id":"aHMv0FD0Eh_W","executionInfo":{"status":"ok","timestamp":1639354161010,"user_tz":-60,"elapsed":244,"user":{"displayName":"Nathan Fiorell","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11892712415418385245"}}},"outputs":[],"source":["# Definition of helper functions\n","\n","def clean_chunk(chunk):\n","    \"\"\" \n","        Cleans dataset chunk by removing unattributed quotes (quotes whose most probable speaker is unknown) or quotes whose speaker name is associated with more \n","        than 1 alias removes 'probas' column and keep only quotes whose speaker probability is greater than 0.6 removes 'phase' column.\n","    \"\"\"          \n","    chunk_clean = chunk.copy()\n","    \n","    # Remove Samples with low numOccurences\n","    chunk_clean = chunk_clean[chunk_clean['numOccurrences'] > min_occurence]\n","\n","    # Filtering for samples containing exactly one QIDs\n","    chunk_clean = chunk_clean.loc[chunk_clean[chunk_clean['qids'].map(len) == 1].index]\n","    chunk_clean['qids'] = chunk_clean['qids'].apply(lambda qids: qids[0])\n","    \n","    # Remove samples with more than 1 speaker \n","    if chunk_clean['probas'].dtype != 'float64': # In case the 'probas' column has already been pre-processed\n","        chunk_clean['probas'] = chunk_clean['probas'].apply(lambda probas: float(probas[0][1]))\n","    \n","    # Remove Samples with low probability\n","    chunk_clean = chunk_clean[chunk_clean['probas'] > min_probability]\n","    \n","    # Removing phase feature\n","    if 'phase' in chunk_clean: # In case the 'phase' column has already been pre-processed\n","        chunk_clean = chunk_clean.drop('phase', axis = 1)\n","\n","    # Removing irrelevant news outlet\n","    # Convert string-formatted list into list\n","    #chunk_clean['urls'] = chunk_clean['urls'].map(lambda x: ast.literal_eval(x))\n","    # Expand each quote into several rows, one for each URL\n","    chunk_clean = chunk_clean.explode('urls')\n","    # Replace each URL by its domain only\n","    chunk_clean['urls'] = chunk_clean['urls'].map(lambda x: urlparse(x).netloc)\n","\n","    domain_count = chunk_clean[['quoteID', 'urls']].groupby(['urls']).count()\n","    single_domain = domain_count[domain_count['quoteID']<10].index.to_list()\n","    quoteID_single_domain = set(df_expanded[df_expanded['urls'].isin(single_domain)]['quoteID']\n","    chunk_clean = chunk_clean.drop(chunk_clean[chunk_clean['quoteID'].isin(quoteID_single_domain)].index)\n","      \n","    return chunk_clean"]},{"cell_type":"code","source":["def describe_chunk(size, i, add_string):\n","    \"\"\"\n","        Print the size of the chunk processed and its number. \n","    \"\"\"\n","    \n","    print(f'{add_string} processing of chunk n°{i+1} with {size} rows')\n","    print(\"\")"],"metadata":{"id":"ZmgQ8-HO_IhR","executionInfo":{"status":"ok","timestamp":1639352283127,"user_tz":-60,"elapsed":262,"user":{"displayName":"Nathan Fiorell","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11892712415418385245"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["def filter_year(path_to_file, chunksize):\n","  df_reader = pd.read_json(path_to_file, lines = True, compression = 'bz2', chunksize = chunksize)\n","\n","  year_df = pd.DataFrame()\n","  i = 0\n","  describe_chunk(chunksize, i, \"Begining\")\n","  for chunk in df_reader:\n","    chunk = clean_chunk(chunk)\n","    year_df = year_df.append(chunk)\n","    describe_chunk(len(chunk), i, \"Ending\")\n","    i = i + 1\n","    describe_chunk(chunksize, i, \"Begining\")\n","  return year_df"],"metadata":{"id":"sIj8rIEcks0w","executionInfo":{"status":"ok","timestamp":1639354393014,"user_tz":-60,"elapsed":250,"user":{"displayName":"Nathan Fiorell","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11892712415418385245"}}},"execution_count":95,"outputs":[]},{"cell_type":"code","source":["chunksize = 10000"],"metadata":{"id":"iMyEC7i7I8w6","executionInfo":{"status":"ok","timestamp":1639354708665,"user_tz":-60,"elapsed":247,"user":{"displayName":"Nathan Fiorell","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11892712415418385245"}}},"execution_count":96,"outputs":[]},{"cell_type":"code","source":["def handle_year(i):\n","  path_to_file = input_path + primer_input + years[i] + tail_input\n","  path_to_output = data_storage_path + primer_output + '/' + primer_output + years[i] + tail_output\n","  year_df = filter_year(path_to_file,chunksize)\n","  year_df.to_csv(path_to_output)"],"metadata":{"id":"pZD7VtF2AZhO","executionInfo":{"status":"ok","timestamp":1639354758430,"user_tz":-60,"elapsed":231,"user":{"displayName":"Nathan Fiorell","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11892712415418385245"}}},"execution_count":102,"outputs":[]},{"cell_type":"code","source":["handle_year(0) #2016"],"metadata":{"id":"Pg5RL47UJ77G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["handle_year(1)#2017"],"metadata":{"id":"R8xzhkB1_up6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["handle_year(2) #2018"],"metadata":{"id":"gXm8zdjK_vzQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["handle_year(3) #2019"],"metadata":{"id":"_7CL1Fqb_xLs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["handle_year(4) #2020"],"metadata":{"id":"-AZSgtTC_2nR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Selection with Hatebase"],"metadata":{"id":"_l-jTySVKwOu"}},{"cell_type":"markdown","source":["## 3.1"],"metadata":{"id":"GcURbdhkLCiM"}},{"cell_type":"code","source":["#Import dataframes\n","path_to_file = data_storage_path + primer_output + '/' + primer_output + years[0] + tail_output\n","df_2016 = pd.read_csv(path_to_file)\n","df_2016['quotation'].str.lower()\n","df_2016.head()"],"metadata":{"id":"4lZpN785AZKC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hate_frame = 'HateBase/hateframe.csv'"],"metadata":{"id":"m_80DOW6BSzq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hate_path = data_storage_path + hate_frame\n","hateframe = pd.read_csv(hate_path)\n","hateset = set(hateframe['Term'])\n","hateset = [' {} '.format(x).lower() for x in hateset]"],"metadata":{"id":"JiCQ3EwpA5Uq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = df_2016_hateful.copy()\n","\n","for col in one_hot: a[col] = 0\n","a['quotation'] = a['quotation'].apply(lambda x: f' {x} ',)\n","a[\"quotation\"] = a['quotation'].str.replace('[^\\w\\s]','')\n","a"],"metadata":{"id":"3D4DhQ9-BvZB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#One hot vector with hate \"domains\"\n","one_hot = list(hateframe.columns)[2:]\n","\n","\n","def extract_hate(df_, hateframe):\n","\n","  out = df_.copy()\n","  for col in one_hot: out[col] = 0\n","\n","\n","  out['quotation'].str.lower()\n","  out['quotation'] = out['quotation'].apply(lambda x: f' {x} ',)\n","  out[\"quotation\"] = out['quotation'].str.replace('[^\\w\\s]','')\n","\n","  \n","  out = out[out['quotation'].str.contains('|'.join(hateset))]\n","\n","  for index, row in out.iterrows():\n","    match = re.search('|'.join(hateset), str(row['quotation']))\n","    if match is not None:\n","      hateword = match[0].strip()\n","      print(hateword)\n","      row[one_hot] = hateframe.query('\"Term\" == @hateword')[one_hot]\n","\n","  return out\n","\n","test = extract_hate(df_2016_hateful, hateframe)\n","\n","  "],"metadata":{"id":"lVxIMK60BxzQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test"],"metadata":{"id":"8z-dO0OnB0Zb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_2016['quotation'].str.lower()\n","df_2016['quotation'] = df_2016['quotation'].apply(lambda x: f' {x} ',)\n","df_2016[\"quotation\"] = df_2016['quotation'].str.replace('[^\\w\\s]','')\n","\n","df_2016_hateful = df_2016[df_2016['quotation'].str.contains('|'.join(hateset))]\n","df_2016_hateful\n"],"metadata":{"id":"VZUQAV5UB2YJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","string = df_2016_hateful['quotation'].iloc[59]\n","\n","re.search('|'.join(hateset), string)[0]"],"metadata":{"id":"v9hgW39nB4UY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in df_2016_hateful['quotation']: print(i)"],"metadata":{"id":"to88qbtdCt1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(29): print(df_2016_hateful['quotation'].iloc[i], '\\n')\n","df_2016_hateful"],"metadata":{"id":"-YUc1TZHCwVX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["phrase = 'He was talking about anyone who feels offended by anything he said,'\n","\n","phrase.contains('|'.join(hateset))"],"metadata":{"id":"bxr9v_WECyjv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.histplot(df_2020['numOccurrences'], log_scale = True)"],"metadata":{"id":"DpMtmXP8C0fS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_2020.describe()"],"metadata":{"id":"ElKxvneVC2BO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We notice that with this first filtering, the median is now at 23 occurences per quote, which tends to prove more the veracity of the quote than the preivous median of 1."],"metadata":{"id":"-DRES8UnC2dl"}},{"cell_type":"markdown","source":["## 4. Sentiment Analysis\n"],"metadata":{"id":"7Ct6tL1pI8P6"}},{"cell_type":"markdown","source":["## 5. Populating speaker information"],"metadata":{"id":"SNmlg4B2JAI-"}},{"cell_type":"markdown","source":["# **Section 2 : Data Analysis**"],"metadata":{"id":"EqK7f1UXJJ1P"}},{"cell_type":"markdown","source":["## 1. Hypotheses"],"metadata":{"id":"Mgh6CfyWJjQf"}}],"metadata":{"colab":{"collapsed_sections":[],"name":"ADA_Project.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}